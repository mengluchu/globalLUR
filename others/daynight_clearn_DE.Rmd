```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=T, warning=FALSE, message=FALSE)
```

Load data
```{r}
set.seed(2)
#load("buffer_oq_dense.R")
#stat = read.csv("stat_values/stat_values.csv")
#nrow(stat)
#names(stat) =  gsub("R_", "ROAD_", names(stat))
#names(stat)= gsub("LONG", "LONGITUDE", names(stat))
#names(stat)= gsub("LAT", "LATITUTE", names(stat))
#names(stat)= gsub("UID", "ID", names(stat))
# m1 =read.csv("/Users/Lu000012/Documents/merged.csv")
 #load( "/Users/Lu000012/Documents/files/GLOBAL_LUR/merged_.Rdata")
 #merged = merge(merged, stat[,-which(names(stat)%in%c("LATITUTE", "LONGITUDE"))], by = "ID", all.x = T)
 load("mergedu.Rdata")
  
#save(merged, file = "merged.Rdata")
#merged = merged [ , -which(grepl("ROAD_|Dis", colnames(merged)))]

```


Required packages
```{r, include=F}
ipak <- function(pkg){
 
   new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
   if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE, repos='http://cran.muenster.r-project.org')
  sapply(pkg, require, character.only = TRUE)
}
packages <- c( "sp","maptools", "MASS" ,"raster", "sf","dplyr", "glmnet","ggplot2" ,"reshape2","lme4" , "tidyr", "RColorBrewer","devtools", "rasterVis","corrplot", "randomForest", "forestFloor","ranger","forecast"  )
ipak(packages)

```

```{r}
  install_github("mengluchu/globalLUR/globalLUR/globalLUR")
 library(globalLUR)
 ls("package:globalLUR")
```

```{r}
spmerged = st_as_sf(merged, coords = c("LONGITUDE","LATITUDE"))
 summary(merged)
library(ggplot2)
plot(spmerged[ "I_1_300"], breaks = c(2e+3, 2e+4, 2e+5,5e+5, 2e+6 ))
plot_error(err = log2(na.interp(spmerged$I_1_3000)), geome = spmerged$geometry,plotraster = T)
plot_error(err = log2(na.interp(spmerged$value_mean)), geome = spmerged$geometry) 
plot_error(err = log2(na.interp(spmerged$I_1_300)), geome = spmerged$geometry,plotraster = T)

```
Get 3 color pallets.
```{r}
colorB = brewer.pal(7,"Greens")
colorG = brewer.pal(11,"PiYG")
colorS = brewer.pal(11, "Spectral")
```

### Data preprocessing:
0) add variables by ID or by rasters (not in this document). 
1) remove unwanted columns or records, 
2) select records (e.g. by country), separate testing and training sets.
3) merge roads 
 

Separate the dataset into training and  test dataset with a fraction (her 80\% of the records are used for training, the rest for testing), "DE" is the two digit for germany. If for world, the sampling uses the fraction per country. 
```{r}
#merged = merge(merged, stat[,-which(names(stat)%in%c("LATITUTE", "LONGITUDE"))], by = "ID", all.x = T)

 
```

```{r}
a= globalLUR::sampledf(merged,fraction = 0.8, "DE" )
plot(with(a$inde_var,day_value-night_value), typ = "h")
abline(h=0)
```

Retrieve test, training, and all variables.  
```{r}
test = a$test
training = a$training
inde_var=a$inde_var
names(inde_var)
```

Merge roads of different road types, the road length of these road types are aggregated. The original road types are substituted (with keep =T, they are remained). 

```{r}

inde_var = merge_roads(inde_var,c(3,4,5), keep = F)
#numeric country
#inde_var$country=as.numeric(inde_var$country)
```

### Basic data exploration 

Remove the country variable, as this is only for one country. 
```{r}
inde_var=inde_var[,-which(grepl("country",names(inde_var)))]
```

Retrieve the predictors and responses as test and training set, basic statistics can be calculated. This step could be done later and with cautious, as these variables are global variables.  
```{r}
# test and training variables
inde_var_test =  inde_var[test,]
inde_var_train =  inde_var[training,]

# predictors
 xtest_f = inde_var[test,-which(grepl("value_mean|day_value|night_value", names(inde_var)))]
 xtrain_f = inde_var[training,-which(grepl("value_mean|day_value|night_value", names(inde_var)))]

# observations  
  y_train_mean = inde_var_train$value_mean 
  y_train_day = inde_var_train$day_value 
  y_train_night = inde_var_train$night_value 
  
  y_test_mean = inde_var_test$value_mean 
  y_test_day = inde_var_test$night_value 
  y_test_night = inde_var_test$night_value 
   
#geo_traing = buffer_oq_dense$geometry[training]
#geo_test = buffer_oq_dense$geometry[test]
 
summary(y_train_mean)
summary(y_train_day)
summary(y_train_night)
```



Plot the paired correlation, for all the variables.  
 
```{r}
 
s= cor( inde_var )
corrplot(s, method = "ellipse" )
```
  

Training and  test dataset size (number of stations )
```{r}
length(y_train_mean)
length(y_test_mean)

```






Checkt uni-variant R square. Caculate teh rsq for day, night and mean, and bind the columns to form a dataframe for plotting. 
```{r}
#plot_rsq(inde_var[,-which(grepl("value_mean|day_|night_",names(inde_var)))],inde_var$value_mean)
#plot_rsq(inde_var[,-which(grepl("value_mean|day_|night_",names(inde_var)))],inde_var$night_value)

#plot_rsq(inde_var[,-which(grepl("value_mean|day_|night_",names(inde_var)))],inde_var$day_value)

rsqmean = univar_rsq(inde_var[,-which(grepl("value_mean|day_|night_",names(inde_var)))],inde_var$value_mean)

rsqday = univar_rsq(inde_var[,-which(grepl("value_mean|day_|night_",names(inde_var)))],inde_var$day_value)

rsqnight = univar_rsq(inde_var[,-which(grepl("value_mean|day_|night_",names(inde_var)))],inde_var$night_value)

rsqdf= cbind(rsqmean,rsqday,rsqnight, rownames(rsqmean))  
names(rsqdf)= c("mean","day","night","vars")

plot_rsq(rsqdf = rsqdf, varname = "vars")

# use different color and manually 
#a5= melt(rsqdf, id = "vars") 

#ggplot(a5, aes(x=vars, y = value , colour= variable)) +
#  geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) + 
#  scale_colour_manual(values=c(colorS[4], colorB[5], colorG[3]))
 
```

The scatter plots between predictors and responses, in sequence mean, day, night
```{r scatterplot}
pre_mat = inde_var_train[,which(grepl("ROAD|pop|value_mean|day_value|night_value|temp|wind|Rsp|OMI|eleva|coast|I_1", names(inde_var_train)))]


scatterplot(pre_mat,"value_mean", "gam")
scatterplot(pre_mat,"day_value", "gam")
scatterplot(pre_mat,"night_value", "gam")
 
 
```



### Modelling
1) LM model
2) Random forest
3) Lasso
4) Mechanical model (nls) 

Extra: 
5) Separate urban/rural hirachical/ two-step linear regression
6) mixed effects regression 

##### LM: linear regression model 

If simply using linear regression, the mean, day, night. Predictors are population, temperature, wind speed, GEOM product, OMI tropo column, elevation, and road buffers. 

i.e. ROAD|population|value_mean|temperature|wind|GEOM product|OMI|elevation. 

Note population is not always significant, though the individual R square for each buffer is high. The prediction for night is much better than for the day


```{r, eval=F}
#(Throw all the predictors in lm, but there are too many irrelavant variables.) 

  # use all variables in LM
   lm3 = lm(y_train_mean~., data = xtrain_f)

```

multiple regression with and without OMI 
```{r}
pre_mat = inde_var_train[,which(grepl("ROAD|pop|value_mean|temp|wind|eleva|coast|I_1", names(inde_var_train )))]
lm1 = lm(value_mean~., data = pre_mat)
summary(lm1)$r.square 
error_matrix(y_test_mean,prediction = predict(lm1,newdata =xtest_f))


pre_mat = inde_var_train[,which(grepl("ROAD|pop|value_mean|temp|wind|eleva|RSp|OMI|coast|I_1", names(inde_var_train )))]
lm1 = lm(value_mean~., data = pre_mat)
summary(lm1)$r.square 

error_matrix(y_test_mean,prediction = predict(lm1,newdata =xtest_f))
 
```

Multiple regression, separating day and night 
```{r}
 


pre_mat = inde_var_train[,which(grepl("ROAD|pop|day_|temp|wind|eleva|RSp|OMI|coast|I_1", names(inde_var_train )))]
lm2 = lm(day_value~., data = pre_mat)
summary(lm2) 
error_matrix(y_test_day,prediction = predict(lm2,newdata =xtest_f))


pre_mat = inde_var_train[,which(grepl("ROAD|pop|night_|temp|wind|eleva|RSp|OMI|coast|I_1", names(inde_var_train )))]
lm3 = lm(night_value~., data = pre_mat)
summary(lm3) 
  error_matrix(y_test_night,prediction = predict(lm3,newdata =xtest_f))
 

```

Testing variable importance: decomposing R square variance
```{r , eval = F}


library("relaimpo", lib.loc="~/R/win-library/3.4")

pre_mat = inde_var_train[,which(grepl("_50|_500|3000|pop|night_|m_1|m_4|m_7|m_11|eleva|OMI|coast", names(inde_var_train )))]
lm2 = lm(night_value~., data = pre_mat)
summary(lm2)
sim =  sort(  calc.relimp(lm2, type =c("pratt"),rela = F)$"pratt",decreasing = T)[1:6]
s2 = melt(as.data.frame(t(sim)))
ggplot(s2,aes(x= variable, y =value))+geom_point()

sim =  sort(  calc.relimp(lm3, type =c("pratt"),rela = F)$"pratt",decreasing = T)[1:6]
s2 = melt(as.data.frame(t(sim)))
ggplot(s2,aes(x= variable, y =value))+geom_point()
```
 


###### Regression tree and random forest. The prediction is so far the best.  For night the prediction error is much lower. Also indicated in the validation. 
```{r}
library(party)
#library(rpart.plot)
#library(partykit)
#set.seed(2)
 
#install_github("mengluchu/globalLUR/globalLUR/globalLUR")
 #library(globalLUR)
# ls("package:globalLUR")

 ctree_LUR(inde_var, y_varname= c("day_value"), training, test, grepstring ="ROAD|pop|temp|wind|Rsp|OMI|eleva|coast|I_1" )



#cforest_LUR(inde_var, y_varname= c("day_value"), training, test, grepstring ="ROAD|pop|temp|wind|Rsp|OMI|eleva|coast" ) # takes really long
#plot(as.simpleparty(a[[2]])) # ctree
#plot(a[[2]],fitmean = T) #ctree party
#prp(a[[3]]) # rpart
#set.seed(2) #randomess of random forest 
 #rf_LUR(inde_var, y_varname= c("day_value"), training, test,  grepstring ="ROAD|pop|temp|wind|Rsp|OMI|eleva|coast" )

 
```

Creates diverse set of trees because
1) trees are instable w.r.t. changes in learning/training data (bagging)
2) randomly preselect mtry splitting variables in each split  

The tree and prediction error will be different if shuffeling training and testing data. 
```{r eval = F}
for (i in 8:2)
{set.seed(i)
a= globalLUR::sampledf(merged,fraction = 0.8, "DE" )
test = a$test
training = a$training
ctree_LUR(inde_var, y_varname= c("day_value"), training, test, grepstring ="ROAD|pop|temp|wind|Rsp|OMI|eleva|coast|I_1" )
}

```


Random forest: cforest is very slow, using ranger. The prediction error should be similar. 
```{r}
 
rf_LUR(inde_var, y_varname= c("day_value"), training, test, grepstring ="ROAD|pop|temp|wind|Rsp|OMI|eleva|coast|I_1" )
```




```{r}
library(caret)
pre_mat = inde_var_train[,which(grepl("ROAD|pop|value_mean|temp|wind|eleva|coast|I_1", names(inde_var_train)))]

 tunegrid <- expand.grid(.mtry=c(33:35), .ntree=c(500, 1000, 1500, 2000, 2500))

 CustomRF <- list(type =  c("Classification", "Regression"), library = "ranger", loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry", "num.trees"), class = rep("numeric", 2), label = c("mtry", "num.tree"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
  ranger(x, y, mtry = param$mtry, num.trees=param$num.trees, ...)
}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes
control <- trainControl(method="cv", number=10)

tgrid <- expand.grid( .num.trees=c(1000, 1500, 2000, 2500))
library(care)
gbmFit1 <- train(value_mean~ ., data = pre_mat,
                 method = 'xgbTree', 
                  trControl=control )

gbmFit1
summary(gbmFit1)
plot(gbmFit1)
```

```{r , eval = F}
 # for testing purpose 
library(GGally)
pre_mat = inde_var_train[,which(grepl("ROAD|pop|value_mean|temp|wind|eleva|coast|I_1", names(inde_var_train)))]

pre_mat = select(inde_var_train, ROAD_2_50, pop3k, value_mean, ROAD_M345_300) 

lm2 = lm(value_mean~., data = pre_mat)

pre_mat2 = select(inde_var_train, ROAD_2_50, pop3k, ROAD_M345_300) 
ggpairs(pre_mat2) 




rf3 <- ranger(value_mean~ ., data = pre_mat, mtry = 33, num.trees = 2000)
rf3
library(pdp)

 
 
p2=partial(rf3, "ROAD_M345_300",plot = TRUE, rug = TRUE)
 
 
p3= partial(rf3, "ROAD_2_50",plot = TRUE, rug = TRUE)


# from 1d to 3d 
p1 = partial(rf3, "pop3k",plot = TRUE, rug = TRUE)


#slow
pd <- partial( rf3, pred.var = c("pop3k", "ROAD_M345_300"  ))

# Default PDP
pd1 = plotPartial(pd)

# Add contour lines and use a different color palette
rwb <- colorRampPalette(c("red", "white", "blue"))
pdp2= plotPartial(pd, contour = TRUE, col.regions = rwb)
 
# 3-D surface
#pdp3 <- plotPartial(pd, levelplot = F, zlab = "ROAD_1_50", colorkey = T, 
 #                   screen = list(z = -20, x = -60) )


# Figure 5
grid.arrange(p1, p2,p3, pd1, pdp2 , ncol = 2)


pre_rf <- predictions(predict(rf3, data =xtest_f ))
#rf_residual <- pre_rf -  rdf_test$NO2
error_matrix(y_test_mean, pre_rf) 
 


#default = randomForest(y_train~ ., data = pre_mat,ntree=5000,mtry=4,
#                         keep.inbag = T,keep.forest = T)
#ff = forestFloor(default, X=pre_mat)
 
#names(pre_mat)
#Col = fcol(ff,cols=1)
#           plot(ff,col=Col,plot_GOF = T)
```


```{r,eval=F}

lmp2=partial(lm2, "ROAD_M345_300",plot = TRUE, rug = TRUE)
 lmp3= partial(lm2, "ROAD_2_50",plot = TRUE, rug = TRUE)
lmp1 = partial(lm2, "pop3k",plot = TRUE, rug = TRUE)
lmpd = partial( lm2, pred.var = c("pop3k", "ROAD_M345_300"  ),plot = TRUE, rug = TRUE)
par(mfrow = c(2,2))
 plot(lmp1)
 plot(lmp2)
 
 plot(lmp3)
 
 plot(lmpd)
grid.arrange(plot(lmpd), plot(lmp1))
```


```{r gradientboostingtree}
library(dismo)
pre_mat = inde_var_train[,which(grepl("ROAD|pop|value_mean|temp|wind|eleva|coast|I_1", names(inde_var_train)))]

pre_mat = select(inde_var_train, ROAD_2_50, pop3k, value_mean, ROAD_M345_300) 

pre_mat2 = select(inde_var_train, ROAD_2_50, pop3k, ROAD_M345_300) 
 

#  rf3 <- gbm.step(data=pre_mat, gbm.x=names(pre_mat2),gbm.y = "value_mean", family="gaussian", tree.complexity = 6, learning.rate = 0.01, bag.fraction = 0.5)
library(gbm)
rf2 =  gbm(formula = value_mean~., data = pre_mat, distribution = "gaussian",
     n.trees = 500,
  interaction.depth = 6,  shrinkage = 0.01,
  bag.fraction = 0.5 )
  
plot(rf2, i.var = 2:3)
plot(rf2, i.var = 1)
plot(rf2, i.var = 2)
plot(rf2, i.var = 3)

summary(rf2)
 
#rf_residual <- pre_rf -  rdf_test$NO2

```


###### LASSO 

In Sequence, mean, day , night. The predicton errors are much higher than random forest, but used a much simpler model 
The variables selected are slightly different from each other. The variables selected each time are also different. 

```{r}

Lasso(inde_var,vis1 = T, y_varname =  "value_mean",training, test)
Lasso(inde_var,vis1 = T, y_varname = "day_value",training, test)
Lasso(inde_var, vis1 = T,y_varname = "night_value",training, test)
 
 
```


##### Mechanical model 

 USing the ring of roads. THe road length in each ring could be normalised by the area of the ring (becomes road length density, not done in this model, can choose normalise =T). However, the coefficents of the regression do not increase or decrease with roadrings further away. Looks the 5000m is dominating. If removing this parameter, the 1000m dominates. The coefficients
 
 
additional variable pop1k, 3k, elevation, sequentially  
```{r}
 
names(inde_var)
buffers_in = c(0,25,50,100,300,500,800,1000,3000)
buffers_out = c(25, 50,100,300,500,800, 1000,3000,5000)
distance_center = (buffers_out-buffers_in)/2 + buffers_in

mechanical(inde_var,"day_value","pop1k", distance_centre = distance_centre, training=training, test = test, norma = F,buffers_in = buffers_in,buffers_out = buffers_out)

mechanical(inde_var,"night_value","pop1k", distance_centre = distance_centre, training=training, test = test, norma = F,buffers_in = buffers_in,buffers_out = buffers_out)

mechanical(inde_var,"day_value","pop3k", distance_centre = distance_centre, training=training, test = test, norma = F,buffers_in = buffers_in,buffers_out = buffers_out)

mechanical(inde_var,"day_value","I_1_300", distance_centre = distance_centre, training=training, test = test, norma = F,buffers_in = buffers_in,buffers_out = buffers_out) 


```
coefficients for different road types as a function of road ring for different road types
```{r}


RDring_coef(inde_var,  quote(value_mean),buffers_in = buffers_in,buffers_out = buffers_out, number_roadtypes = 3) 
RDring_coef(inde_var,  quote(day_value),buffers_in = buffers_in,buffers_out = buffers_out, number_roadtypes = 3) 
RDring_coef(inde_var,  quote(night_value),buffers_in = buffers_in,buffers_out = buffers_out, number_roadtypes = 3) 
 
```
coefficients for different road types as a function of road ring for different road types, with pop3k as a variable, for day polution
```{r}
RDring_coef(inde_var,  quote(day_value), pop_var = "pop3k",buffers_in = buffers_in,buffers_out = buffers_out, number_roadtypes = 3) 
 
 
```

 
 
```{r, eval = F}
library(nlstools)
overview(a1)
 
a1resi= nlsResiduals(a1)
plot(a1resi)
 
```
 
```{r}
library(rpart)
fit = rpart(value_mean~., data = pre_mat)
printcp(fit)
a =  randomForest(iris[,-5], iris[,5], ntree=10)
plot(a)
```

