---
title: "Opengeohub summer school 2019: Global air pollution modeling."
author: "Meng Lu, Utrecht University"
date: "Aug. 7, 2020"
output:
  html_document:
    fig_height: 8
    fig_width: 12
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path = 'Figs_geohub/',
                      echo=T, include = T, warning = FALSE, message = FALSE)
```
 
#### The tutorial shows the processes of data exploration and data analysis for the global air pollution modeling project. The statistical learning methods used include random forest, stochastic gradient boosting, extreme gradient boosting, and a mechanical model using nonlinear regression. The partial dependence plot and variable importance are visualised to partly interpretate models.   

Required packages
```{r, include=F}
ipak <- function(pkg){
 
   new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
   if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}
#repos='http://cran.muenster.r-project.org'

stdata = c("sp", "sf", "raster")
Stat_methods = c("glmnet", "ranger", "gbm", "xgboost", "party", "caret", "gstat")
visual = c("RColorBrewer", "ggplot2", "corrplot", "tmap", "leaflet", "mapview", "pdp", "vip", "DT", "sparkline")
map = c("maptools")
tidy = c("devtools", "dplyr",  "tidyr", "reshape2", "knitr")
other = c("countrycode", "htmlwidgets", "data.table", "Matrix")
 

packages <- c(stdata, tidy, Stat_methods, visual, map, other)
ipak(packages)

```



Auxilary package with preprocessed data in dataframe.

```{r}
install_github("mengluchu/globalLUR/globalLUR/globalLUR")
library(globalLUR)
ls("package:globalLUR")
```

If the above is not successeful for MacOS users, the following and a restart of R may be needed 
```{r, eval = F}
system('defaults write org.R-project.R force.LANG en_US.UTF-8')
```



#### Preparation

Get 3 color pallets.
```{r}
colorB = brewer.pal(7, "Greens")
colorG = brewer.pal(11, "PiYG")
colorS = brewer.pal(11, "Spectral")
```

```{r}
#set.seed(2)
```
 
Dataset:

value_mean: annual mean NO2, day_value: annual mean NO2 at daytime, night_value: annual mean NO2 at night time.   
road_XX_size: road lenght within a buffer with radius "size" of type XX.    
I_size: Industrial area within a buffer with radius "size".   
Tropomi_2018: TROPOMI averaged over Feb 2018 - Jan 2019.    
temperature_2m_m: monthly mean temperature at 2m height of month "m".  
wind_speed_10m_m:monthly mean wind speed at 2m height of month "m".  
pop1k/ 3k /5k: population 1, 3, 5 km resolution.  
Rsp: Surface remote sensing and chemical transport model product.  
OMI_mean_filt: OMI column density, 2017 annual average.    

```{r}
# add data usethis::use_data()
data("merged")
names(data)
data("countrywithppm") # countries with ppm (parts per million)
summary(merged)
datatable(merged, rownames = FALSE, filter = "top", options = list(pageLength = 5, scrollX = T))
```

Fill missing data with NA
```{r}
merged_1 = na_if(merged,-1)
```

Merge roads of road type 3, 4, 5, the road length of these road types are aggregated. The original road types are substituted (with keep =T, they are remained). 

 
```{r mergeroads}
merged_mr = merge_roads(merged_1, c(3, 4, 5), keep = F) # keep = T keeps the original roads. 
names(merged_mr)
#numeric country
#inde_var$country=as.numeric(inde_var$country)
```


### Visualization

Visualize with tmap: convenient
```{r}
locations_sf = st_as_sf(merged_mr, coords = c("LONGITUDE","LATITUDE"))
osm_valuemean = tm_shape(locations_sf) +
  tm_dots( "value_mean", col = "value_mean", size = 0.05,
     popup.vars = c("value_mean", "day_value", "night_value", "ROAD_2_100", "ROAD_2_5000")) + tm_view(basemaps = c('OpenStreetMap'))
#+tm_shape(lnd)+tm_lines()
tmap_save(osm_valuemean, "NO2mean.html")
```

Visualize with leaflet: more control.
show Day/night ratio, red: day/night >1, blue, day/nigh <1 

```{r}
merged_fp = merged_mr %>% mutate(ratiodn = day_value/night_value) %>% mutate(color = ifelse(ratiodn >1, "red", "blue"))
m  = leaflet(merged_fp) %>%
     addTiles() %>% addCircleMarkers(radius = ~value_mean/5, color = ~color, popup =   ~as.character(value_mean),fill = FALSE) %>% addProviderTiles(providers$Esri.NatGeoWorldMap) %>% addMouseCoordinates() %>%  
addHomeButton(ext = extent(116.2, 117, 39.7, 40), layer.name = "Beijing") %>% addHomeButton(ext = extent(5, 5.2, 52, 52.2), layer.name = "Utrecht")
saveWidget(m, file = "NO2daynight.html")
```

Boxplot

```{r}
countryname = paste(merged_mr$country, countrycode(merged_mr$country, 'iso2c', 'country.name'), sep = ":") 

#tag country with ppm 
countryname_s_e=ifelse( merged_mr$country %in% countrywithppm[countrywithppm %in% merged_mr$country], paste(countryname,"*", sep = ""), countryname)
merged_mr$countryfullname = countryname_s_e

# use the median for colour
mergedmedian = merged_mr %>% group_by(country) %>% mutate(median =  median(value_mean, na.rm = TRUE))

bp2 <- ggplot(mergedmedian, aes(x=countryfullname, y=value_mean, group=country)) +
  labs(x = "Country", y = expression(paste("NO"[2], "  ", mu, "g/", "m"^3)), cex = 1.5) +
  geom_boxplot(aes(fill = median)) + 
  theme(text = element_text(size = 13), axis.text.x = element_text(angle = 90, hjust = 1)) +   scale_fill_distiller(palette = "Spectral")
#   scale_color_brewer(direction = 1)
print(bp2 + ylim(0, 100))
```


 
 
Plot the paired correlation, for  road predictors, population, Tropomi.  For DE, CN, and world
 
```{r}
merged_mr %>% na.omit %>% filter(country == "DE") %>% dplyr::select(matches("_value|ROAD|pop|Trop")) %>% cor %>% corrplot(type = "upper", method = "pie", tl.cex = 0.7)
    
merged_mr %>% na.omit %>% filter(country == "CN") %>% dplyr::select(matches("_value|ROAD|pop|Trop")) %>% cor %>% corrplot(type = "upper", method = "pie", tl.cex = 0.7)
       
merged_mr %>% na.omit %>% dplyr::select(matches("_value|ROAD|pop|Trop")) %>% cor %>% corrplot(type = "upper", method = "pie", tl.cex = 0.7)
```


Spatial dependency
```{r}
grd_sp <- as_Spatial(locations_sf)
dt.vgm = variogram(value_mean~1, grd_sp)
plot(dt.vgm)
 
dt.vgm = variogram(value_mean~1, grd_sp, cutoff = 10)
plot(dt.vgm)

countryvariogram = function(COUN, cutoff){
loca =  locations_sf%>%filter(country == COUN)
grd_sp <- as_Spatial(loca)

dt.vgm = variogram(value_mean~1, grd_sp, cutoff = cutoff)
plot(dt.vgm)
}
 
countryvariogram("DE", 1)
countryvariogram("US", 1)
countryvariogram("CN", 1)
 
#Moran I test
#install.packages("ape", dependencies = TRUE)
#library(ape)

#merged_mrf =  merged_mr%>%filter(country == "US")
#no2.dists <- as.matrix(dist(cbind(merged_mrf$LONGITUDE, merged_mrf$LATITUDE)))
#no2.dists[1:5, 1:5]
#no2.inv <- 1/no2.dists
#diag(no2.inv) <- 0
#no2.inv[1:5, 1:5]
#Moran.I(merged_mrf$value_mean, na.rm = T, no2.inv) 
```

spatial correlation of errors of random forest

```{r residual_correlation}
set.seed(2)
pr = globalLUR::sampledf(merged_mr, fraction = 0.8, country2digit = "CN", grepstring_rm = "ID|countryfullname")
inde_var_train = with(pr, inde_var[training, ])
inde_var_test = with(pr, inde_var[test, ])
pre_mat = subset_grep(inde_var_train, grepstring = "ROAD|pop|value_mean|temp|wind|eleva|coast|I_1|Trop")
rf = ranger(value_mean~ ., data = pre_mat, mtry = 33, num.trees = 2000, importance = "permutation")
rf
errordf = with(inde_var_test, data.frame(error = predictions(predict(rf, inde_var_test)) - value_mean, LONGITUDE = LONGITUDE, LATITUDE = LATITUDE))
                 
error_sp = errordf %>% st_as_sf(coords = c("LONGITUDE","LATITUDE")) %>% as_Spatial
dt.vgm = variogram(error~1, error_sp, cutoff = 1)
plot(dt.vgm)
```

### Data preprocessing:
0) add variables by ID or by rasters (not in this document). 
1) remove unwanted columns or records, 
2) select records (e.g. by country), separate testing and training sets.
 
Separate the dataset into training and  test dataset with a fraction (her 80\% of the records are used for training, the rest for testing), "DE" is the two digit for germany. If for world, the sampling uses the fraction per country. 
```{r}
#merged = merge(merged, stat[,-which(names(stat)%in%c("LATITUTE", "LONGITUDE"))], by = "ID", all.x = T)
```


#### Germany as an example

```{r sample}
response_predictor = globalLUR::sampledf(merged_mr, fraction = 0.8, country2digit = "DE", grepstring_rm = "ID|LATITUDE|LONGITUDE|countryfullname")

#Retrieve test, training, and all variables.  
 
test = response_predictor$test
training = response_predictor$training
inde_var = response_predictor$inde_var
inde_var = inde_var %>% dplyr::select(-country)
```


The size of test and training dataset
```{r size}
length(test)
length(training)
```

 
```{r eval=F }
#Checkt uni-variant R square. Caculate the r-sq for day, night and mean, and bind the columns to form a dataframe for plotting.  
rsqmean =  inde_var %>% dplyr::select(-matches("value_mean|day_|night_")) %>%  univar_rsq (inde_var$value_mean)

rsqday = inde_var %>% dplyr::select(-matches("value_mean|day_|night_")) %>%  univar_rsq (inde_var$day_value) 
rsqnight = inde_var %>% dplyr::select(-matches("value_mean|day_|night_")) %>%  univar_rsq (inde_var$night_value) 

rsqdf = cbind(rsqmean, rsqday, rsqnight, rownames(rsqmean))  
names(rsqdf)= c("mean","day","night","vars")

plot_rsq(rsqdf = rsqdf, varname = "vars",xlab = "predictors", ylab = "R-squared")
```

The scatter plots between predictors and responses, mean 
```{r scatterplot}
inde_var %>% dplyr::select(matches("pop1k|Tro|wind_speed_10m_7|temperature_2m_7|day_value")) %>% scatterplot(y_name = "day_value", fittingmethod = "loess")
inde_var %>% dplyr::select(matches("ROAD_1|day_value")) %>% scatterplot(y_name = "day_value", fittingmethod = "gam")
inde_var %>% dplyr::select(matches("ROAD_2|day_value")) %>% scatterplot(y_name = "day_value", fittingmethod = "loess")
inde_var %>% dplyr::select(matches("ROAD_M345|day_value")) %>% scatterplot(y_name = "day_value", fittingmethod = "loess")
# can choose any variable to look at the scatterplot
#scatterplot(inde_var, "night_value", "gam")
#scatterplot(inde_var,"value_mean", "gam" )
```
### Modelling
1) Tree based
2) Lasso
3) Mechanical model (nls) 

Extra: 
5) Separate urban/rural hirachical/ two-step linear regression
6) mixed effects regression 

##### LM: linear regression model 

If simply using linear regression, the mean, day, night. Predictors are population, temperature, wind speed, GEOM product, OMI tropo column, elevation, and road buffers. 

i.e. ROAD|population|value_mean|temperature|wind|GEOM product|OMI|elevation. 

Note population is not always significant, though the individual R square for each buffer is high. The prediction for night is much better than for the day

###### Regression tree
The tree and prediction error will be different if shuffeling training and testing data. 
```{r eval = T}
for (i in 2:5)
{set.seed(i)
testtree = globalLUR::sampledf(merged_mr,fraction = 0.8, "DE" )
with (testtree, ctree_LUR(inde_var, y_varname= c("day_value"), training = training, test = test, grepstring ="ROAD|pop|temp|wind|Rsp|OMI|eleva|coast|I_1|Tropomi" ))
}

```



###### random forest.

Creates diverse set of trees because
1) trees are instable w.r.t. changes in learning/training data (bagging)
2) randomly preselect mtry splitting variables in each split  

model training and parameter tuning
```{r}
#caret
names(getModelInfo())
```

Training RF using Caret

Mtry and number of trees
```{r}
inde_var_train = subset_grep(inde_var[training, ], "ROAD|pop|temp|wind|Rsp|OMI|eleva|coast|I_1|Tropomi|value_mean")
model_rf = train(value_mean ~ ., data = inde_var_train, method='rf') # mtry
plot(model_rf)

fitted <- predict(model_rf, inde_var[test, ])
error_matrix(prediction = fitted, validation = inde_var[test, ]$value_mean)
```


```{r, eval = F}
model_gbm= train(value_mean ~ ., data = inde_var[training, ], method='gbm')
plot(model_gbm)
#gbm.step optimal number of trees. 
```





```{r ensemble, eval=FALSE}
#install.packages("caretEnsemble")
library(caretEnsemble)

# Stacking Algorithms - Run multiple algos in one call.
trainControl <- trainControl(method = "repeatedcv", 
                             number = 10, 
                             repeats = 2,
                             savePredictions = TRUE, 
                             classProbs = TRUE)

algorithmList <- c('rf', 'adaboost', 'earth', 'xgbDART', 'svmRadial')

set.seed(100)
models <- caretList(value_mean ~ ., data = inde_var_train, trControl = trainControl, methodList = algorithmList) 
results <- resamples(models)
summary(results)
```

#### Important variables and Partial plots: using the "vip" package
 
```{r}
vip::list_metrics()

pre_mat = subset_grep(inde_var_train, grepstring = "ROAD|pop|value_mean|temp|wind|eleva|coast|I_1|Trop")
rf = ranger(value_mean~ ., data = pre_mat, mtry = 33, num.trees = 2000,importance = "permutation")
rf
# ranger method
importance(rf)

#vip
DF_P_r2 = vi(rf, method = "permute", target = "value_mean", metric = "r2" ) # very clear what method is used and what is the target
DF_P_rmse = vi(rf, method = "permute", target = "value_mean", metric = "rmse") 

vip (DF_P_rmse)
vip (DF_P_r2)
```

 
partial dependence plots: all the variables. (using sparklines takes a while)
```{r pdp, eval = F}
library(DT)
library(sparkline)
a=add_sparklines(DF, fit = rf)
library(htmlwidgets)
saveWidget(a, file="sparkline.html")
```


Partial dependence plot of selected variables
```{r}
library(GGally)
pre_mat_s = inde_var_train %>% select(value_mean, ROAD_2_50, pop3k, ROAD_M345_300) 

 
lm_s = lm(value_mean~., data = pre_mat_s)

rf_s = ranger(value_mean~ ., data = pre_mat_s, num.trees = 2000, importance = "permutation")
rf_s
```

correlation 
```{r}
pre_mat_predictor = pre_mat_s%>%select(-value_mean) 
ggpairs(pre_mat_predictor)
```


```{r}
p_lm = partial(lm_s, "ROAD_M345_300",plot = TRUE, rug = TRUE)
plot(p_lm)

p2 = partial(rf_s, "ROAD_M345_300",plot = TRUE, rug = TRUE)
plot(p2)
```

```{r}
#slow
pd <- partial(rf_s, pred.var = c("pop3k", "ROAD_M345_300"  ))

# Default PDP
pd1 = plotPartial(pd)

# Add contour lines and use a different color palette
rwb <- colorRampPalette(c("red", "white", "blue"))
pdp2 = plotPartial(pd, contour = TRUE, col.regions = rwb)
 
# 3-D surface
#pdp3 <- plotPartial(pd, levelplot = F, zlab = "ROAD_1_50", colorkey = T, 
#                   screen = list(z = -20, x = -60) )
 
p3 = partial(rf_s, "ROAD_2_50", plot = TRUE, rug = TRUE)
p1 = partial(rf_s, "pop3k", plot = TRUE, rug = TRUE)
grid.arrange(p1, p2, p3, pd1, pdp2, ncol = 2)

```

 
 
#### Gradient boosting

```{r gradientboostingtree}

pre_mat = subset_grep(inde_var_train, grepstring = "ROAD|pop|value_mean|temp|wind|eleva|coast|I_1|Trop")

gbm1 =  gbm(formula = value_mean~., data = pre_mat, distribution = "gaussian", n.trees = 2000,
  interaction.depth = 6, shrinkage = 0.01, bag.fraction = 0.5 )
names(pre_mat)
summary(gbm1)

plot(gbm1, i.var = 2:3)
#plot(gbm1, i.var = 1) 
#rf_residual <- pre_rf -  rdf_test$NO2
```

Xgboost

Tunning XGBoost is more complex (as it has a lot more hyperparameters to tune):
https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/


1. gamma[default=0][range: (0,Inf)]
It controls regularization (or prevents overfitting). The optimal value of gamma depends on the data set and other parameter values.
Higher the value, higher the regularization. Regularization means penalizing large coefficients which don't improve the model's performance. default = 0 means no regularization.
Tune trick: Start with 0 and check CV error rate. If you see train error >>> test error, bring gamma into action.  

2. lambda and Alpha

3. nrounds[default=100]
It controls the maximum number of iterations. For classification, it is similar to the number of trees to grow.
Should be tuned using CV

4. eta[default=0.3][range: (0,1)]
It controls the learning rate, i.e., the rate at which our model learns patterns in data. After every round, it shrinks the feature weights to reach the best optimum.
Lower eta leads to slower computation. It must be supported by increase in nrounds.
Typically, it lies between 0.01 - 0.3

5. max_depth[default=6][range: (0,Inf)]
It controls the depth of the tree.
Larger data sets require deep trees to learn the rules from data.
Should be tuned using CV

```{r XGBOOST}
  y_varname= "day_value"
  varstring = "ROAD|pop|temp|wind|RSp|OMI|eleva|coast|I_1|Tropo"
  prenres = paste(y_varname, "|", varstring, sep = "")
  sub_mat = subset_grep(inde_var, prenres)
    
  x_train = sub_mat[training, ]
  y_train = sub_mat[training, y_varname]
    
  x_test = sub_mat[test, ]
  y_test = sub_mat[test, y_varname]
 
  df_train = data.table(x_train, keep.rownames = F)
  df_test = data.table(x_test, keep.rownames = F)
  formu = as.formula(paste(y_varname, "~.", sep = ""))
  dfmatrix_train = sparse.model.matrix(formu, data = df_train)[, -1]
  dfmatrix_test = sparse.model.matrix(formu, data = df_test)[, -1]
 
  train_b = xgb.DMatrix(data = dfmatrix_train, label = y_train) 
  test_b = xgb.DMatrix(data = dfmatrix_test, label = y_test) 
  params <- list(booster = "gbtree",max_depth = 4,
  eta = 0.05,
  nthread = 2,
  nrounds = 1000,
  Gamma = 2)
  #xgb_t = xgb.train (params = params, data = train_b, nrounds = 500, watchlist = list(val=test_b, train = train_b), print_every_n = 10, early_stopping_rounds = 50, maximize = F , eval_metric = "rmse")

  #outputvec = inde_var[training, y_varname]
  max_depth = 4
  eta = 0.01
  nthread = 4
  nrounds = 1000
  Gamma = 2
  
  #simplest: tunning of rounds 
  xgbcv <- xgb.cv( data = train_b, nfold = 10, nround =  nrounds, eta = eta, nthread = nthread, Gamma = Gamma,showsd = T, stratified = T, print_every_n = 200, early_stopping_rounds = 50, maximize = F)
  xgbcv
  str(xgbcv)
  bst <- xgboost(data = train_b, max_depth = max_depth, eta = eta, nthread = nthread, nrounds = xgbcv$best_iteration, Gamma = Gamma, verbose = 1, print_every_n = 200, early_stopping_rounds = 50, maximize = F )
   
  xgbpre = predict(bst, test_b)
  error_matrix(y_test, xgbpre)
```

```{r}
varstring = "ROAD|pop|temp|wind|RSp|OMI|eleva|coast|I_1|Tropo"
xgboost_LUR(inde_var, max_depth =4, eta =0.02, nthread = 2, nrounds = 2000, y_varname= c("day_value"),training = training, test = test, grepstring = varstring )

#xgboost_imp (variabledf = inde_var, y_varname = "day_value", max_depth = 5, eta = 0.02, nthread = 4, nrounds = 2000, training = training, test = test, grepstring = varstring )
```

###### LASSO 

In Sequence, mean, day , night. The predicton errors are much higher than random forest, but used a much simpler model 
The variables selected are slightly different from each other. The variables selected each time are also different. 

```{r}
Lasso(inde_var, vis1 = T, y_varname = "day_value", training = training, test=test)
Lasso(inde_var, vis1 = T,y_varname = "night_value", training = training, test=test)
```

 
##### Mechanical model 

 USing the ring of roads. THe road length in each ring could be normalised by the area of the ring (becomes road length density, not done in this model, can choose normalise =T). However, the coefficents of the regression do not increase or decrease with roadrings further away. Looks the 5000m is dominating. If removing this parameter, the 1000m dominates. The coefficients
 
 
coefficients for different road types as a function of road ring for different road types
```{r}
names(inde_var)
buffers_in = c(0, 25, 50, 100, 300, 500, 800, 1000, 3000)
buffers_out = c(25, 50, 100, 300, 500, 800, 1000, 3000, 5000)
RDring_coef(inde_var, quote(day_value), buffers_in = buffers_in,buffers_out = buffers_out, number_roadtypes = 3) 
```

additional variable pop1k 
```{r}
distance_center = (buffers_out-buffers_in)/2 + buffers_in
mechanical(inde_var,"day_value","pop1k", distance_centre = distance_centre, training = training, test = test, norma = F,buffers_in = buffers_in,buffers_out = buffers_out)
```



 

```{r eval=FALSE}
install.packages("sperrorest")
library(sperrorest)
CN_df = merged_mr %>% filter(country == "CN")%>%dplyr::select(-day_value, -value_mean, -ID, -country)
names(CN_df)

mypred_part <- function(object, newdata) predictions(predict(object, newdata)) 
f1 =  value_mean ~ ROAD_M345_300 + pop1k + ROAD_2_1000  
CN_sptest= sperrorest(formula =f1, 
                      data = CN_df, 
                      model_fun = ranger,
                      coords = c("LONGITUDE", "LATITUDE"),
                      model_args = list( num.trees = 2000),
                      pred_fun = mypred_part,
                      progress = TRUE, 
                      smp_args = list(repetition = 1 , nfold =10))

resamp <- partition_factor_cv(CN_df, nfold = 5, repetition = 1:1,coords = c("LONGITUDE", "LATITUDE"), fac = "Tropomi_2018")
plot(resamp,CN_df,coords = c("LONGITUDE", "LATITUDE"))
plot(resamp, maipo, coords = c("utmx","utmy")
```

 
